# """VGG.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1TYQ64NeYdPYiZx-GFdj3lYMg0U-XhdWc
# """

# ----MODEL-APPLIED-----
# ---CNN---GradientBoosting---LogisticRegression---DecisionTree---RandomForest---LSTM---


from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

df = pd.read_csv("/content/drive/MyDrive/malimg_df.csv")

X = df.iloc[:,:-1]
y = df.iloc[:,-1:]

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

num_classes = len(np.unique(y))

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# -----># CNN Applied

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, LSTM, Flatten
from sklearn.model_selection import train_test_split

X_train_CNN = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_CNN = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

model_CNN = Sequential([
    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_CNN.shape[1], X_train_CNN.shape[2])),
    MaxPooling1D(pool_size=2),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model_CNN.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
history_CNN = model_CNN.fit(X_train_CNN, y_train, epochs=100, batch_size=32, validation_data=(X_test_CNN, y_test))

train_loss = history_CNN.history['loss']
val_loss = history_CNN.history['val_loss']
train_accuracy = history_CNN.history['accuracy']
val_accuracy = history_CNN.history['val_accuracy']

import matplotlib.pyplot as plt

plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
plt.plot(train_accuracy, label='Training Accuracy')
plt.plot(val_accuracy, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Epochs vs Accuracy')
plt.show()

# ----->Gradient Boost Applied

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

from sklearn.datasets import make_classification

dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

params = {
    'objective': 'multi:softmax',
    'num_class': 25,
    'eval_metric': ['mlogloss', 'merror']
}

eval_results = {}
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtrain, 'train'), (dtest, 'test')],
                  evals_result=eval_results, verbose_eval=10)

train_logloss = eval_results['train']['mlogloss']
test_logloss = eval_results['test']['mlogloss']

plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 2)
plt.plot(boosting_rounds, train_logloss, label='Training Loss')
plt.plot(boosting_rounds, test_logloss, label='Validation Loss')
plt.xlabel('Boosting Rounds')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()

plt.tight_layout()
plt.show()

iterations = []
accuracies = []
predicted_probs = []

model = xgb.XGBClassifier(objective='multi:softmax', num_class=num_classes, eval_metric='mlogloss', random_state=42)
model.fit(X_train, y_train)

for i in range(1, 101):
    iterations.append(i)
    y_pred = model.predict(X_test)
    accuracies.append(accuracy_score(y_test, y_pred))
    probs = model.predict_proba(X_test)[:, 1]
    predicted_probs.append(np.mean(probs))

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

conf_matrix = confusion_matrix(y_test, y_pred)

# Display the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.arange(25), yticklabels=np.arange(25))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ----->Logistic Regression Applied

from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression(solver='lbfgs', multi_class='multinomial',max_iter=100)

X_train.shape, y_train.shape, X_test.shape, y_test.shape

lr_model.fit(X_train,y_train)

y_pred_lr = lr_model.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred_lr)
print(f"Accuracy: {accuracy}")

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

conf_matrix = confusion_matrix(y_test, y_pred_lr)

# Display the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.arange(25), yticklabels=np.arange(25))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

iterations = []
accuracies = []
predicted_probs = []

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
lr_epoch_model = LogisticRegression(solver='lbfgs', multi_class='multinomial',max_iter=1000)

for i in range(1, 101):  # Change the range according to your iteration needs
    lr_epoch_model.fit(X_train, y_train)
    iterations.append(i)
    accuracies.append(lr_epoch_model.score(X_test, y_test))
    probs = lr_epoch_model.predict_proba(X_test)[:, 1]  # Assuming binary classification
    predicted_probs.append(np.mean(probs))  # Store mean predicted probability

plt.figure(figsize=(10, 5))

# Plot accuracy
plt.plot(iterations, accuracies, label='Accuracy', color='blue')
plt.xlabel('Iterations')
plt.ylabel('Accuracy')
plt.title('Iterations vs Accuracy (Logistic Regression)')
plt.grid(True)
plt.legend()

# Plot predicted probabilities
plt.figure(figsize=(10, 5))
plt.plot(iterations, predicted_probs, label='Predicted Probabilities', color='red')
plt.xlabel('Iterations')
plt.ylabel('Mean Predicted Probability')
plt.title('Iterations vs Mean Predicted Probability (Logistic Regression)')
plt.grid(True)
plt.legend()

plt.tight_layout()
plt.show()

# ----->Decision Tree Applied

from sklearn.tree import DecisionTreeClassifier
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)
y_pred_dt = dt_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_dt)
print("Accuracy:", accuracy)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred_dt)

# Display the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.arange(25), yticklabels=np.arange(25))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ----->Random Forest Applied

from sklearn.decomposition import PCA
pca = PCA(n_components=32)
X_red = pca.fit_transform(X)

X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_red, y, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=512, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_rf)
print("Accuracy:", accuracy)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_test, y_pred_rf)

# Display the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=np.arange(25), yticklabels=np.arange(25))
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# ----->LSTM

X = df.iloc[:,:-1]
y = df.iloc[:,-1:]

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

from sklearn.decomposition import PCA
pca = PCA(n_components=512)

X_red = pca.fit_transform(X)

X_red_train, X_red_test, y_red_train, y_red_test = train_test_split(X_red, y, test_size=0.2, random_state=42)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

X_reshaped = X_red_train.reshape(X_red_train.shape[0], 1, X_red_train.shape[1])
X_test_reshaped = X_red_test.reshape(X_red_test.shape[0], 1, X_red_test.shape[1])

model = Sequential()
model.add(LSTM(64, input_shape=(X_reshaped.shape[1], X_reshaped.shape[2])))
model.add(Dense(25, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_reshaped, y_train, epochs=100, batch_size=32, validation_data=(X_test_reshaped, y_test))

import matplotlib.pyplot as plt
train_acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
train_loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(train_acc) + 1)

plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, train_acc, label='Training Accuracy')
plt.plot(epochs, val_acc, label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()